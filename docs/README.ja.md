<p align="center">
  <a href="../README.md">English</a> ·
  <a href="README.zh-CN.md">中文</a> ·
  <a href="README.pl.md">Polski</a> ·
  <a href="README.es.md">Español</a> ·
  <strong>日本語</strong> ·
  <a href="README.ko.md">한국어</a> ·
  <a href="README.ru.md">Русский</a> ·
  <a href="README.fr.md">Français</a> ·
  <a href="README.de.md">Deutsch</a>
</p>

# RobotSumo RL トレーニングシステム

このプロジェクトは、強化学習（Actor-Criticアーキテクチャ）を用いて訓練された自律型RobotSumo戦闘エージェントを実装しています。システムは**セルフプレイ機構**を備えた専用トレーニング環境を使用しており、学習エージェントは「マスター」モデルや自身の過去バージョンと対戦することで、戦闘戦略を継続的に進化・洗練させます。

主な特徴には、攻撃的な前進、正確な狙い、戦略的なリング内ポジショニングを促進し、回転や後退運転のような受動的行動を抑制する高度な**報酬シェイピングエンジン**が含まれます。

### *リアルタイム戦闘デモ（報酬のライブ追跡付き）*

https://github.com/user-attachments/assets/ca0baaf4-f6bf-412e-9ca7-3786b3346c5d
<p align="center">
  <em>SACエージェント（緑） vs A2Cエージェント（青）</em>
</p>

https://github.com/user-attachments/assets/2b496931-9eda-4c8b-88ca-7286d5fa9b42
<p align="center">
  <em>SACエージェント（緑） vs PPOエージェント（青）</em>
</p>

https://github.com/user-attachments/assets/bdabd7a4-4890-47b2-a4cf-d7549b31da2e
<p align="center">
  <em>A2Cエージェント（緑） vs PPOエージェント（青）</em>
</p>


## システムアーキテクチャ

以下のブロック図は、閉ループ制御システムを示しています。**モバイルロボット**（物理/センサ層）と**RLコントローラ**（意思決定層）を区別しています。目標信号 $\mathbf{r}_t$ は、報酬エンジンを介してポリシーを形成するため、訓練フェーズ中にのみ使用されます。

<div align="center">
  <img src="../resources/control_loop.png" width="650px">
</div>

### 機能ブロック

* **コントローラ（RLポリシー）:** SAC、PPO、A2C などのニューラルネットワークベースのエージェントで、現在の観測ベクトルを連続的な行動空間にマッピングします。デプロイ時には推論エンジンとして機能します。
* **ダイナミクス:** ロボットの二次物理モデルを表します。入力力やトルクに対する応答を計算し、質量、慣性モーメント、摩擦を考慮し、外部の**摂動**（SAT衝突）の影響を受けます。
* **キネマティクス:** 状態統合ブロックで、一般化速度をグローバル座標に変換します。ロボットの位置姿勢をアリーナの原点に対して維持します。
* **センサフュージョン（知覚）:** ロボット状態ベクトル、生のグローバル状態データ、環境情報（例：相手の位置）を正規化された自己中心的観測ベクトルに変換する前処理レイヤです。

### 信号ベクトル

ブロック間の通信は、以下の数学的ベクトルで定義されます：

* $\mathbf{r}_t$: **報酬/目標信号** – 訓練中のみ使用され、報酬シェイピング関数を介してポリシー最適化を導きます。
* $\mathbf{a}_t = [v\_{target}, \omega\_{target}]^T$: **行動ベクトル** – 希望する線速度および角速度を表す制御コマンド。
* $\dot{\mathbf{x}}_t = [\dot{x}, \dot{y}, \dot{\theta}]^T$: **状態微分** – ダイナミクスエンジンによって計算される瞬時の一般化速度。
* $\mathbf{y}_t = [x, y, \theta]^T$: **物理出力（姿勢）** – グローバル座標系におけるロボットの現在の座標と向き。
* $\mathbf{s}_t$: **観測ベクトル (`state_vec`)** – 速度などの固有受容情報と、相手やアリーナ端までの距離など外界情報を含む11次元正規化特徴ベクトル。


## 状態ベクトル仕様
入力状態ベクトル (`state_vec`) は11個の正規化された値で構成されており、エージェントにアリーナ上の状況を包括的に把握させます。

| インデックス | パラメータ | 説明 | 範囲 | ソース / センサ |
| :--- | :--- | :--- | :--- | :--- |
| 0 | `v_linear` | ロボットの線速度（前進/後退） | [-1.0, 1.0] | ホイールエンコーダ / IMU融合 |
| 1 | `v_side` | ロボットの横方向速度 | [-1.0, 1.0] | IMU（加速度計） / 状態推定 |
| 2 | `omega` | 回転速度 | [-1.0, 1.0] | ホイールエンコーダ / ジャイロスコープ（IMU） |
| 3 | `pos_x` | アリーナ上のX座標 | [-1.0, 1.0] | オドメトリ / 位置推定融合 |
| 4 | `pos_y` | アリーナ上のY座標 | [-1.0, 1.0] | オドメトリ / 位置推定融合 |
| 5 | `dist_opp` | 相手までの正規化距離 | [0.0, 1.0] | 距離センサ（IR/超音波） / LiDAR |
| 6 | `sin_to_opp` | 相手への角度のサイン | [-1.0, 1.0] | 幾何計算（距離センサに基づく） |
| 7 | `cos_to_opp` | 相手への角度のコサイン | [-1.0, 1.0] | 幾何計算（距離センサに基づく） |
| 8 | `dist_edge` | アリーナ端までの距離 | [0.0, 1.0] | 床面センサ（ライン検出器） / 幾何計算 |
| 9 | `sin_to_center` | アリーナ中心に対する方向（サイン） | [-1.0, 1.0] | ラインセンサ / 状態推定 + 幾何計算 |
| 10 | `cos_to_center` | アリーナ中心に対する方向（コサイン） | [-1.0, 1.0] | ラインセンサ / 状態推定 + 幾何計算 |

## 報酬シェイピングの詳細
報酬システムは、攻撃的な戦闘行動と戦略的生存を促進するよう設計されています：

* **終端報酬（Terminal Rewards）:** 勝利時の大きなボーナス、場外落下や時間切れ（引き分け）時の大きなペナルティ。  
* **後退ブロック:** 後退走行は厳しくペナルティされ、そのステップの他の報酬を打ち消します。
* **アンチスピニング:** 無目的な回転を防ぐための過剰回転ペナルティ。
* **前進報酬:** 前進行動の報酬は、相手に正確に向いているかどうかでスケーリングされます。
* **運動的関与（Kinetic Engagement）:** 相手に正面を向きながら前進速度を維持した場合の高倍率ボーナスで、決定的な攻撃を促します。
* **エッジセーフティ:** 落下方向への移動にペナルティを与え、アリーナ中央への回帰を報酬。
* **戦闘ダイナミクス:** 高速正面衝突（押す行為）に報酬、側面や後方からの衝撃にはペナルティ。
* **効率:** 各ステップに一定の時間ペナルティを課し、最速での勝利を促進。


## 環境仕様
シミュレーション環境は、公式のRobotSumo競技規格を高い物理的忠実度で再現するよう構築されています。

* **アリーナ:** 
    * **(土俵 / Dohyo):** 標準半径（77 cm）と定義済み中心点でモデル化。環境は境界条件を厳密に適用し、ロボットのシャーシのいずれかの角が `ARENA_DIAMETER_M` を超えた時点で試合は終了（終端状態）。
* **ロボット物理:** 
    * **シャーシ:** ロボットは10x10 cmの正方形寸法 (`ROBOT_SIDE`) に準拠。
    * **ダイナミクス:** 質量に基づく加速度、回転慣性、摩擦モデルを実装（タイヤグリップをシミュレートするための横摩擦も含む）。
* **衝突システム:** リアルタイム接触処理は **分離軸定理 (SAT)** によって行われます。非弾性の重なりを計算し、物理的インパルスを適用。ロボットの質量と反発係数に基づき、前方および横方向の速度に影響。
* **開始条件:** 標準的な開始距離（アリーナ半径の約70%）を使用。固定位置および360度ランダム方向に対応し、学習の堅牢性を向上。

## パフォーマンス分析 & ベンチマーク

トーナメント結果は、戦闘戦略の進化と異なる強化学習アーキテクチャの効率を明確に示しています。比較により、ピーク性能および収束速度の両方に明確な階層が確認できます。

### トーナメント順位表 & 効率

| 順位 | エージェント | モデルバージョン | ELOレーティング | 必要エピソード数 |
|:----:|:------------:|:----------------:|:--------------:|:----------------:|
| 1-5  | **SAC** | v19 - v23 | **1391 - 1614** | **~378** |
| 6-10 | **PPO** | v41 - v45 | **1128 - 1342** | **~1,049** |
| 11-15| **A2C** | v423 - v427 | **791 - 949** | **10,000 - 24,604** |

> [!NOTE]
> **収束速度に関する注記:** アーキテクチャ間でサンプル効率に大きな差があります。SACはピーク性能に到達するのが大幅に早く、PPOの約3分の1、A2Cの60分の1以下のエピソード数で熟練レベルの戦闘に収束しました。


### トップモデル比較
*各アーキテクチャの最終バージョン（最高性能）を比較。*

<table width="100%">
  <tr>
    <td align="center">
      <img src="../resources/peak_elo_comparison_algos.png" width="800px"><br>
      <em>アルゴリズム別ピークELO</em>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="../resources/peak_elo_ranking_models.png" width="800px"><br>
      <em>ピークモデル</em>
    </td>
  </tr>
</table>

---

### 進化の進捗
*学習プロセス全体を通じて定期的にサンプリングされたモデルの性能分析（各アーキテクチャ5段階）。*

<table width="100%">
  <tr>
    <td align="center">
      <img src="../resources/sampled_elo_comparison_algos.png" width="800px"><br>
      <em>アルゴリズム別サンプルモデルの平均ELO</em>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="../resources/sampled_elo_ranking_models.png" width="800px"><br>
      <em>サンプルモデル</em>
    </td>
  </tr>
</table>

### 主なポイント

* **SAC (Soft Actor-Critic) の効率:** この環境でSACは圧倒的な勝者です。オフポリシーの最大エントロピー枠組みにより、最高技能レベル（1614 ELO）に最も効率よく到達。
    * *行動上の注記:* SACエージェントは、位置を崩された場合でも向きを巧みに回復し、相手の小さなポジションミスさえも積極的に利用する能力を発達させました。
* **PPO の安定性と戦術:** PPOは信頼できる競争者として安定した学習と競争力のある性能を提供。SACより低いELOでプラトーに達しますが、連続制御タスクには堅牢な選択肢です。
    * *行動上の注記:* 興味深いことに、PPOエージェントは「クリンチ」状況で優れた戦術的操作を学び、接近戦で相手のバランスを崩して位置的優位を得ることができました。
* **A2Cの性能ギャップ:** 基本的なAdvantage Actor-Criticアルゴリズムはサンプル効率と安定性で大きく苦戦。十分な学習を行っても、より高度なアーキテクチャの初期ELOを下回る性能に留まり、単純なオンポリシー手法の限界を示しています。
* **アーキテクチャの進化:** このプロジェクトは、現代のオフポリシー手法（SAC）が従来のオンポリシー手法よりも**連続的で非線形な制御タスク**に適していることを示しています。SACはオフポリシーデータから学習しながらエントロピーを最大化できるため、より高度で適応的な戦闘行動と、はるかに高い性能上限を実現します。


## 簡単な開始方法

シミュレーションを実行し、エージェントの動作を確認するには、以下の手順に従ってください。

### インストール
```bash
make install
```
### クイックデモ（クロスプレイ、例: SAC vs PPO）
```bash
make cross-play
```

### その他のコマンド
```bash
make train-sac        # 新規SACトレーニングを開始（既存モデルをクリア）
make train-ppo        # 新規PPOトレーニングを開始（既存モデルをクリア）
make train-a2c        # 新規A2Cトレーニングを開始（既存モデルをクリア）
make test-sac         # 専用SACテストスクリプトを実行
make test-ppo         # 専用PPOテストスクリプトを実行
make test-a2c         # 専用A2Cテストスクリプトを実行
make tournament       # トップ5の学習済みモデルを自動選択してELOランキングを実行
make clean-models     # すべての学習履歴とマスターモデルを削除
```
*利用可能な自動化ターゲットの完全なリストについては、[Makefile](../Makefile)を参照してください。*


## 将来の改善の可能性

* **観測ノイズの導入**：LiDARやオドメトリセンサーに対してガウスノイズモデルを実装し、現実世界のセンサーの確率的変動をシミュレートすることで、ポリシーの一般化能力と堅牢性を向上させます。
* **入力状態ベクトルの拡張**：最近のLiDARセンサーデータに基づき、相手の推定速度を入力状態ベクトルに追加することで、予測的な戦闘操作を改善します。
* **高度な物理モデリング**：ホイールのスリップ、線形-角速度の飽和、モーター飽和などの非線形ダイナミクスを実装し、現実世界の物理制約をより正確にシミュレートしてSim-to-Real性能を向上させます。
* **自動解析と統計**：モデルの意思決定を解析し、詳細なメトリクス（例：ラウンドあたりの平均ステップ数、回転頻度、リアエンドや側面衝突などの衝突タイプ）を生成するスクリプトを作成します。
* **アブレーションスタディ**：報酬形成関数のパラメータ化を行い、アブレーションスタディを実施。位置取りと攻撃性など個々の要素がSACやPPOの安定性や収束にどのように寄与するかを解析します。
* **評価環境と回帰テスト**：固定戦術シナリオ（例：エッジ回復チャレンジ、特定の初期向き）を用いた回帰テストスイートを開発し、新しいモデルバージョンが基礎スキルを損なわずにELO向上を最適化できることを保証します。

## 引用

このリポジトリが研究に役立った場合、以下のように引用できます：

**APAスタイル**
> Brzustowicz, S. (2026). RobotSumo-RL: Reinforcement Learning for sumo robots using SAC, PPO, A2C algorithms (Version 1.0.0) [Source code]. https://github.com/sebastianbrzustowicz/RobotSumo-RL

**BibTeX**
```bibtex
@software{brzustowicz_robotsumo_rl_2026,
  author = {Sebastian Brzustowicz},
  title = {RobotSumo-RL: Reinforcement Learning for sumo robots using SAC, PPO, A2C algorithms},
  url = {https://github.com/sebastianbrzustowicz/RobotSumo-RL},
  version = {1.0.0},
  year = {2026}
}
```
> [!TIP]
> サイドバーの **"Cite this repository"** ボタンを使用すると、これらの引用を自動でコピーしたり、メタデータファイルをダウンロードしたりできます。

## ライセンス

RobotSumo-RL ソース利用可能ライセンス（AI使用不可）
詳細は [LICENSE](../LICENSE) ファイルを参照してください。

## 著者

Sebastian Brzustowicz &lt;Se.Brzustowicz@gmail.com&gt;
