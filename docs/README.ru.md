<p align="center">
  <a href="../README.md">English</a> ·
  <a href="README.zh-CN.md">中文</a> ·
  <a href="README.pl.md">Polski</a> ·
  <a href="README.es.md">Español</a> ·
  <a href="README.ja.md">日本語</a> ·
  <a href="README.ko.md">한국어</a> ·
  <strong>Русский</strong> ·
  <a href="README.fr.md">Français</a> ·
  <a href="README.de.md">Deutsch</a>
</p>

# Система обучения Robot Sumo RL

> [!IMPORTANT]
>  Реализация State-of-the-Art (SOTA): По состоянию на январь 2026 года, этот репозиторий представляет собой самый продвинутый фреймворк с открытым исходным кодом для боев Robot Sumo. Это первое решение, предлагающее комплексный бенчмарк алгоритмов SAC, PPO и A2C, интегрированных с механизмом соревновательного самообучения (self-play).

Этот проект реализует автономного боевого агента Robot Sumo, обучаемого с использованием методов **обучения с подкреплением** (архитектура Actor-Critic). Система использует специализированную среду обучения с механизмом **self-play**, в котором обучаемый агент соревнуется с "Master"-моделью или своими историческими версиями, чтобы постоянно развивать и совершенствовать свои боевые стратегии.  

Ключевые особенности включают продвинутый **движок формирования наград (reward shaping engine)**, который стимулирует агрессивное движение вперед, точное прицеливание и стратегическое позиционирование на арене, одновременно штрафуя пассивное поведение, такое как вращение на месте или движение назад.

### *Демонстрация боя в реальном времени с отслеживанием наград.*

https://github.com/user-attachments/assets/ca0baaf4-f6bf-412e-9ca7-3786b3346c5d
<p align="center">
  <em>Агент SAC (зеленый) vs агент A2C (синий)</em>
</p>

https://github.com/user-attachments/assets/2b496931-9eda-4c8b-88ca-7286d5fa9b42
<p align="center">
  <em>Агент SAC (зеленый) vs агент PPO (синий)</em>
</p>

https://github.com/user-attachments/assets/bdabd7a4-4890-47b2-a4cf-d7549b31da2e
<p align="center">
  <em>Агент A2C (зеленый) vs агент PPO (синий)</em>
</p>


## Архитектура системы

Следующая блок-схема иллюстрирует систему управления с замкнутым контуром. Она разделяет **Мобильного робота** (физический/сенсорный уровень) и **RL-контроллер** (уровень принятия решений). Обратите внимание, что сигнал цели $\mathbf{r}_t$ используется исключительно во время фазы обучения для формирования политики через движок наград.

<div align="center">
  <img src="../resources/control_loop.png" width="650px">
</div>

### Функциональные блоки

* **Контроллер (RL Policy):** Агент на основе нейронной сети (например, SAC, PPO или A2C), который отображает текущий вектор наблюдений в непрерывное пространство действий. Работает как движок вывода во время эксплуатации.
* **Динамика:** Представляет физическую модель второго порядка робота. Вычисляет отклик на входные силы и моменты, учитывая массу, момент инерции и трение, под влиянием внешних **возмущений** (столкновения SAT).
* **Кинематика:** Блок интеграции состояния, преобразующий обобщённые скорости в глобальные координаты. Поддерживает позу робота относительно начала арены.
* **Слияние данных сенсоров (Perception):** Слой предварительной обработки, который преобразует вектор состояния робота, сырые глобальные данные и информацию об окружении (например, положение соперника) в нормализованный эгоцентрический вектор наблюдений.

### Векторные сигналы

Обмен информацией между блоками определяется следующими математическими векторами:

* $\mathbf{r}_t$: **Сигнал награды/цели** – используется исключительно во время обучения для оптимизации политики через функцию формирования наград.
* $\mathbf{a}_t = [v\_{target}, \omega\_{target}]^T$: **Вектор действий** – команды управления, представляющие желаемые линейную и угловую скорости.
* $\dot{\mathbf{x}}_t = [\dot{x}, \dot{y}, \dot{\theta}]^T$: **Производная состояния** – мгновенные обобщённые скорости, вычисленные движком динамики.
* $\mathbf{y}_t = [x, y, \theta]^T$: **Физический выход (Позa)** – текущие координаты и ориентация робота в глобальной системе координат.
* $\mathbf{s}_t$: **Вектор наблюдений (`state_vec`)** – 11-мерный нормализованный вектор признаков, содержащий проприоцептивные сигналы (скорость) и экстероцептивные пространственные отношения (расстояние до соперника/краёв арены).

## Спецификация вектора состояния
Входной вектор состояния (`state_vec`) состоит из 11 нормализованных значений, предоставляя агенту полный обзор ситуации на арене:

| Индекс | Параметр | Описание | Диапазон | Источник / Сенсор |
| :--- | :--- | :--- | :--- | :--- |
| 0 | `v_linear` | Линейная скорость робота (вперед/назад) | [-1.0, 1.0] | Энкодеры колес / Интеграция с IMU |
| 1 | `v_side` | Боковая скорость робота | [-1.0, 1.0] | IMU (акселерометр) / Оценка состояния |
| 2 | `omega` | Угловая скорость | [-1.0, 1.0] | Энкодеры колес / Гироскоп (IMU) |
| 3 | `pos_x` | Положение по X на арене | [-1.0, 1.0] | Одометрия / Интеграция локализации |
| 4 | `pos_y` | Положение по Y на арене | [-1.0, 1.0] | Одометрия / Интеграция локализации |
| 5 | `dist_opp` | Нормализованное расстояние до противника | [0.0, 1.0] | Датчики расстояния (IR/Ультразвук) / LiDAR |
| 6 | `sin_to_opp` | Синус угла на противника | [-1.0, 1.0] | Геометрия (на основе датчиков расстояния) |
| 7 | `cos_to_opp` | Косинус угла на противника | [-1.0, 1.0] | Геометрия (на основе датчиков расстояния) |
| 8 | `dist_edge` | Расстояние до ближайшего края арены | [0.0, 1.0] | Датчики пола (линейные детекторы) / Геометрия |
| 9 | `sin_to_center` | Направление относительно центра арены | [-1.0, 1.0] | Линейные датчики / Оценка состояния + Геометрия |
| 10 | `cos_to_center` | Направление относительно центра арены | [-1.0, 1.0] | Линейные датчики / Оценка состояния + Геометрия |

## Детали формирования наград
Система наград разработана для поощрения агрессивного ведения боя и стратегического выживания:

* **Конечные награды:** Большие бонусы за победу и существенные штрафы за вылет за пределы арены или ничью по времени.  
* **Блокировка заднего хода:** Движение назад строго штрафуется и аннулирует другие награды на этом шаге.
* **Анти-вращение:** Штрафы за чрезмерное вращение, чтобы предотвратить бессмысленное кружение.
* **Продвижение вперед:** Награды за движение вперед масштабируются с учетом точности прицеливания (ориентация на противника).
* **Кинетическое взаимодействие:** Высокие бонусы за сохранение прямой скорости при прямом столкновении с противником, стимулируя решительные атаки.
* **Безопасность на краю:** Проактивная логика, штрафующая движение к краю арены и поощряющая возврат к центру.
* **Динамика боя:** Награды за столкновения лоб в лоб на высокой скорости и штрафы за удары с боков или сзади.
* **Эффективность:** Постоянный штраф за время на каждом шаге, чтобы стимулировать максимально быстрый выигрыш.

## Спецификация среды

Среда симуляции создана с учетом официальных стандартов соревнований Robot Sumo и высокой физической достоверности:

* **Арена:** 
    * **(Дохё):** Смоделирована с стандартным радиусом (77 см) и определенной центральной точкой. Среда строго соблюдает граничные условия; матч завершается (Конечное состояние), как только любой угол шасси робота выходит за пределы `ARENA_DIAMETER_M`.     
* **Физика робота:** 
    * **Шасси:** Роботы соответствуют квадратным размерам 10x10 см (`ROBOT_SIDE`).
    * **Динамика:** Система реализует модели ускорения на основе массы, вращательной инерции и трения (включая боковое трение для имитации сцепления колес).
* **Система столкновений:** Обработка контактов в реальном времени реализована с помощью **Теоремы разделяющих осей (SAT)**. Она вычисляет неупругие наложения и применяет физические импульсы, влияя на продольные и боковые скорости с учетом массы роботов и коэффициента восстановления.
* **Стартовые условия:** Предусмотрено стандартное стартовое расстояние (~70% радиуса арены) с поддержкой как фиксированных позиций, так и случайной ориентации на 360° для повышения устойчивости обучения.

## Анализ производительности и бенчмарки

Результаты турнира наглядно демонстрируют эволюцию боевых стратегий и эффективность различных архитектур обучения с подкреплением. Сравнение показывает явную иерархию как по пиковым результатам, так и по скорости сходимости.

### Турнирная таблица и эффективность

| Место | Агент | Версии модели | Рейтинг ELO | Необходимое количество эпизодов |
|:----:|:-----:|:--------------:|:----------:|:-----------------:|
| 1-5  | **SAC** | v19 - v23 | **1391 - 1614** | **~378** |
| 6-10 | **PPO** | v41 - v45 | **1128 - 1342** | **~1,049** |
| 11-15| **A2C** | v423 - v427| **791 - 949** | **10,000 - 24,604** |

> [!NOTE]
> **Примечание о скорости сходимости:** Существует значительная разница в эффективности выборки между архитектурами. SAC достиг своего пикового потенциала значительно раньше, требуя примерно **в 3 раза меньше эпизодов**, чем PPO, и более **в 60 раз меньше**, чем A2C, чтобы достичь уровня боевой компетентности.

### Сравнение лучших моделей
*Сравнение версий с наивысшей производительностью (финальные итерации) для каждой архитектуры.*

<table width="100%">
  <tr>
    <td align="center">
      <img src="../resources/peak_elo_comparison_algos.png" width="800px"><br>
      <em>Пиковый рейтинг ELO по алгоритму</em>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="../resources/peak_elo_ranking_models.png" width="800px"><br>
      <em>Пиковые модели</em>
    </td>
  </tr>
</table>

---

### Эволюционный прогресс
*Анализ производительности моделей, отобранных через регулярные интервалы на протяжении всего процесса обучения (5 этапов для каждой архитектуры).*

<table width="100%">
  <tr>
    <td align="center">
      <img src="../resources/sampled_elo_comparison_algos.png" width="800px"><br>
      <em>Средний рейтинг ELO отобранных моделей по алгоритму</em>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="../resources/sampled_elo_ranking_models.png" width="800px"><br>
      <em>Отобранные модели</em>
    </td>
  </tr>
</table>

### Основные выводы

* **Эффективность SAC (Soft Actor-Critic):** SAC является бесспорным лидером в этой среде. Его off-policy подход с максимальной энтропией позволил достичь высочайшего уровня мастерства (1614 ELO) с лучшей эффективностью использования выборки.
    * *Замечание по поведению:* Агенты SAC разработали сложную способность восстанавливать ориентацию при смещении и активно использовать даже незначительные ошибки позиционирования противника.
* **Стабильность и тактика PPO:** PPO остается надежным конкурентом, обеспечивая стабильное обучение и конкурентоспособную производительность. Хотя он достигает плато на более низком уровне ELO, чем SAC, он по-прежнему является устойчивым выбором для задач непрерывного управления.
    * *Замечание по поведению:* Интересно, что агенты PPO проявили себя особенно хорошо в ситуациях «клинча», обучаясь тактическим маневрам для вывода противника из равновесия при близком контакте и получения преимущества в позиционировании.
* **Разрыв в производительности A2C:** Базовый алгоритм Advantage Actor-Critic значительно отставал по эффективности использования выборки и стабильности. Даже при обширном обучении его производительность оставалась ниже начального уровня ELO более продвинутых архитектур, что подчеркивает ограничения простых on-policy методов в этой задаче.
* **Эволюция архитектуры:** Проект демонстрирует, что современные off-policy методы (SAC) гораздо лучше подходят для **непрерывных, нелинейных задач управления**, чем традиционные on-policy методы. Способность SAC максимизировать энтропию при обучении на off-policy данных ведет к более сложным, адаптивным боевым стратегиям и значительно более высокому потолку производительности.


## Быстрый старт

Чтобы запустить симуляцию и увидеть агентов в действии, выполните следующие шаги:

### Установка
```bash
make install
```
### Быстрая демонстрация (Cross-Play, например, SAC vs PPO)
```bash
make cross-play
```

### Другие команды
```bash
make train-sac        # Запускает новое обучение SAC (очищает старые модели)
make train-ppo        # Запускает новое обучение PPO (очищает старые модели)
make train-a2c        # Запускает новое обучение A2C (очищает старые модели)
make test-sac         # Запускает специализированный тест SAC
make test-ppo         # Запускает специализированный тест PPO
make test-a2c         # Запускает специализированный тест A2C
make tournament       # Автоматически выбирает топ-5 обученных моделей и проводит рейтинг ELO
make clean-models     # Удаляет всю историю обучения и мастер-модели
```
*Для полного списка доступных целей автоматизации смотрите [Makefile](../Makefile).*


## Потенциальные будущие улучшения

* **Внедрение шума в наблюдения:** Реализация гауссовых моделей шума для лидаров и одометрии, чтобы смоделировать стохастику сенсоров в реальном мире, улучшая обобщаемость и устойчивость политики.
* **Расширение входного состояния:** Добавление в вектор состояния оценки скорости противника на основе последних измерений лидаром для улучшения прогнозируемых боевых манёвров.
* **Продвинутое моделирование физики:** Реализация нелинейной динамики, такой как проскальзывание колёс, насыщение линейно-угловой скорости и насыщение моторов, для более точного моделирования физических ограничений и повышения потенциала Sim-to-Real.
* **Автоматизированная аналитика и статистика:** Создание скрипта для анализа решений модели и генерации подробных метрик (например, среднее количество шагов за раунд, частота вращения, типы столкновений: сзади или боковые).
* **Aбляционные исследования:** Параметризация функции формирования награды для проведения ablation-исследований, чтобы изолировать, как отдельные компоненты (например, позиционирование vs. агрессия) влияют на стабильность и сходимость SAC и PPO.
* **Оценочные сценарии и регрессионное тестирование:** Разработка набора фиксированных тактических сценариев (например, задачи по возврату с края арены, определённые стартовые ориентации) для регрессионного тестирования, чтобы новые версии модели не теряли базовые навыки при оптимизации для более высокого ELO.

## Цитирование

Если этот репозиторий помог вам в исследовании, вы можете сослаться на него:

**APA стиль**
> Brzustowicz, S. (2026). Robot-Sumo-RL: Reinforcement Learning for sumo robots using SAC, PPO, A2C algorithms (Version 1.0.0) [Source code]. https://github.com/sebastianbrzustowicz/Robot-Sumo-RL

**BibTeX**
```bibtex
@software{brzustowicz_robot_sumo_rl_2026,
  author = {Sebastian Brzustowicz},
  title = {Robot-Sumo-RL: Reinforcement Learning for sumo robots using SAC, PPO, A2C algorithms},
  url = {https://github.com/sebastianbrzustowicz/Robot-Sumo-RL},
  version = {1.0.0},
  year = {2026}
}
```
> [!TIP]
> Вы также можете использовать кнопку **"Cite this repository"** в боковой панели для автоматического копирования этих ссылок или скачивания файла с метаданными.

## Лицензия

Лицензия Robot-Sumo-RL с доступом к исходному коду (использование AI запрещено).
Полные условия и ограничения см. в файле [LICENSE](../LICENSE).

## Лицензия

Sebastian Brzustowicz &lt;Se.Brzustowicz@gmail.com&gt;
